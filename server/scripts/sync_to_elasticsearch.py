#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°†è¯—è¯æ•°æ®åŒæ­¥åˆ° Elasticsearch
"""

import asyncio
import sys
import io
from pathlib import Path

# è®¾ç½®æ ‡å‡†è¾“å‡ºä¸ºUTF-8ç¼–ç ï¼ˆWindowså…¼å®¹ï¼‰
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from sqlalchemy import select
from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker, AsyncSession
from elasticsearch import Elasticsearch, helpers

from app.core.config import settings
from app.models.author import Author
from app.models.poetry import Poetry


async def sync_to_es(batch_size: int = 1000):
    """åŒæ­¥æ•°æ®åˆ° Elasticsearch"""

    # è¿æ¥æ•°æ®åº“
    engine = create_async_engine(
        settings.DATABASE_URL,
        echo=False,
        pool_pre_ping=True,
    )

    async_session = async_sessionmaker(
        engine,
        class_=AsyncSession,
        expire_on_commit=False,
    )

    # è¿æ¥ Elasticsearch
    es = Elasticsearch([settings.ELASTICSEARCH_URL])

    print("=" * 70)
    print("ğŸ”„ åŒæ­¥è¯—è¯æ•°æ®åˆ° Elasticsearch")
    print("=" * 70)

    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
    index_name = "poetry_index"

    if es.indices.exists(index=index_name):
        print(f"\nâš ï¸  ç´¢å¼• {index_name} å·²å­˜åœ¨")
        response = input("æ˜¯å¦åˆ é™¤ç°æœ‰ç´¢å¼•å¹¶é‡æ–°åˆ›å»º? (y/n): ")
        if response.lower() == 'y':
            es.indices.delete(index=index_name)
            print(f"âœ… å·²åˆ é™¤ç´¢å¼• {index_name}")
        else:
            print("â­ï¸  è·³è¿‡ç´¢å¼•åˆ›å»º")

    # åˆ›å»ºç´¢å¼•
    if not es.indices.exists(index=index_name):
        print(f"\nğŸ“ åˆ›å»ºç´¢å¼• {index_name}...")

        index_body = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "analysis": {
                    "analyzer": {
                        "ik_analyzer": {
                            "type": "standard"
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "id": {"type": "long"},
                    "title": {"type": "text"},
                    "content": {"type": "text"},
                    "author_name": {"type": "text"},
                    "author_id": {"type": "long"},
                    "dynasty": {"type": "keyword"},
                    "type": {"type": "keyword"},
                    "tags": {"type": "keyword"}
                }
            }
        }

        es.indices.create(index=index_name, body=index_body)
        print(f"âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ")

    # è·å–æ‰€æœ‰è¯—è¯
    async with async_session() as session:
        print(f"\nğŸ“– è¯»å–è¯—è¯æ•°æ®...")

        result = await session.execute(
            select(Poetry, Author.name)
            .outerjoin(Author, Poetry.author_id == Author.id)
        )

        poetries = result.all()
        total = len(poetries)

        print(f"âœ… å…±è¯»å– {total:,} é¦–è¯—è¯")

        # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®
        print(f"\nğŸ”„ å¼€å§‹åŒæ­¥åˆ° Elasticsearch (æ‰¹æ¬¡å¤§å°: {batch_size})...")

        actions = []
        synced = 0

        for poetry, author_name in poetries:
            doc = {
                "_index": index_name,
                "_id": str(poetry.id),
                "_source": {
                    "id": poetry.id,
                    "title": poetry.title,
                    "content": poetry.content,
                    "author_name": author_name or "ä½šå",
                    "author_id": poetry.author_id,
                    "dynasty": poetry.dynasty,
                    "type": poetry.type,
                    "tags": poetry.tags or []
                }
            }

            actions.append(doc)

            # æ¯è¾¾åˆ°æ‰¹æ¬¡å¤§å°å°±æ‰§è¡Œä¸€æ¬¡æ‰¹é‡æ’å…¥
            if len(actions) >= batch_size:
                helpers.bulk(es, actions)
                synced += len(actions)
                print(f"   å·²åŒæ­¥ {synced:,} / {total:,} ({synced/total*100:.1f}%)")
                actions = []

        # æ’å…¥å‰©ä½™æ•°æ®
        if actions:
            helpers.bulk(es, actions)
            synced += len(actions)
            print(f"   å·²åŒæ­¥ {synced:,} / {total:,} ({synced/total*100:.1f}%)")

    # åˆ·æ–°ç´¢å¼•
    es.indices.refresh(index=index_name)

    # éªŒè¯æ•°æ®
    count = es.count(index=index_name)['count']
    print(f"\nâœ… åŒæ­¥å®Œæˆï¼")
    print(f"   Elasticsearch ä¸­çš„æ–‡æ¡£æ•°: {count:,}")

    # æµ‹è¯•æœç´¢
    print(f"\nğŸ” æµ‹è¯•æœç´¢...")
    search_result = es.search(
        index=index_name,
        body={
            "query": {
                "multi_match": {
                    "query": "æ˜¥çœ ",
                    "fields": ["title", "content", "author_name"]
                }
            },
            "size": 3
        }
    )

    hits = search_result['hits']['hits']
    print(f"   æœç´¢ 'æ˜¥çœ ' æ‰¾åˆ° {search_result['hits']['total']['value']} ä¸ªç»“æœ")

    if hits:
        print(f"\n   å‰3ä¸ªç»“æœ:")
        for idx, hit in enumerate(hits, 1):
            source = hit['_source']
            print(f"   {idx}. ã€Š{source['title']}ã€‹ - {source['author_name']}")

    await engine.dispose()
    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼")


if __name__ == "__main__":
    print("=" * 70)
    print("ğŸš€ æ˜Ÿè¯­è¯—è¯å¹³å° - Elasticsearch åŒæ­¥å·¥å…·")
    print("=" * 70)
    print()

    try:
        asyncio.run(sync_to_es())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­åŒæ­¥")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ åŒæ­¥è¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
